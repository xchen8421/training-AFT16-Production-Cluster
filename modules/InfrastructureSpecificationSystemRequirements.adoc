:imagesdir: ../images/
:source-highlighter: rouge
:icons: font


= Infrastructure Specification : System Requirements


{sp} +
{sp} +
{sp} +


image::color_logo.png[align="center",pdfwidth=75%]


{sp}+



[cols="5a,1a,14a",grid="none",frame="none"]
|===
|

{sp}+
{sp}+

image::learning-objectives.svg[pdfwidth=90%]
|
|
By completing this module, you will be able to:

* You'll be able to document the number of required Confluent components, the hardware requirements for every node, and in doing so, verify that the infrastructure will meet both functional and non-functional requirements.

|===


Before you begin, you should have a good understanding of:

* Data usage and cluster architecture plans

* Information on several operational items: 

  Total number of partitions in the cluster

  Amount of data ingress and egress for the cluster and its network bandwidth

  Number of applications or services accessing the cluster

  Types of queries or calls to be made by applications

  Data retention policies

  Deployment environment (e.g., cloud provider)

  Application availability requirements

Data usage and cluster architecture plans are needed before this task can be completed. Information on several operational items is required to complete the utilization estimates.

== Why is it important?

It is important to create a Confluent deployment with the correct hardware specifications and correct number of nodes for each component of the platform.

== Who does this involve?

* Enterprise Architects

* Platform owners

* Infrastructure team

== When does it matter?

A correctly sized cluster will ensure that the target performance and reliability requirements defined by the SLA are met. You should define a cluster sizing strategy before:

* Ordering hardware

* Cluster deployments

== Create your system requirements 

Decide which cloud provider you want to spin up the confluent cloud cluster if using Confluent Cloud.

* AWS
* Google Cloud
* Microsoft Azure
* Internet enabled cluster

**What type of cluster do you need:**

* **A production cluster:** The single datacenter design provides robust protection against broker failure. If a client application is using a certain broker for connectivity to the cluster, and that broker fails, another bootstrap broker can provide connectivity to the cluster. However, this single Kafka deployment is vulnerable if the entire datacenter fails. A Standard or Dedicated cluster would work best for this type.

* **A geographically redundant disaster recovery cluster:** Ensures in case of catastrophic hardware failure, software failure, power outage, denial of service attack, or any other event that causes one datacenter to completely fail—Kafka continues running in another datacenter until service is restored. Crucial for essential services.  A Dedicated cluster is best for this type.

* **A pre-production cluster:** Typically with the same deployment characteristics as the production cluster. A Basic cluster would be a more economically option, but Standard is also an option. 

* **A dev and/or an integration cluster:** A cluster for testing with Confluent Platform and Cloud. A Basic cluster works best for this type.

.

You will need to understand how to size the following Confluent components separately, you can use the sizing calculator or you can make your own spreadsheet. Here are the hardware requirements for each component:

**Apache ZooKeeper**

* Nodes: 3-5
* Storage: Transaction log: 512 GB / Storage: 2 X 1 TB SATA, RAID 10
* Memory: 4 GB RAM
* CPU: 2-4 cores

**Broker**

* Nodes: 3
* Storage: 12 X 1 TB disk. RAID 10 is optional. Separate OS disks from Apache Kafka® storage
* Memory: 64 GB RAM
* CPU: Dual 12 core sockets

**Kafka Connect**

* Nodes: 2
* Storage: Only required for installation
* Memory: 0.5 - 4 GB heap size depending on connectors
* CPU: Typically not CPU- bound. More cores is better than faster cores.

**Confluent Schema Registry** 

* Nodes: 2
* Storage: Only required for installation
* Memory: 1 GB heap size
* CPU: Typically not CPU- bound. More cores is better than faster cores.

**ksqlDB**

* Nodes: 2
* Storage: Use SSD. Sizing depends on the number of concurrent queries and the aggregation performed.
* Memory: 20 GB RAM
* CPU: 4 cores

**Control Center**

* Nodes: 1
* Storage: 300 GB, preferably SSDs
* Memory: 32 GB RAM (JVM default 6 GB)
* CPU: 12 cores or more

**Control Center - Reduced infrastructure mode**

* Nodes: 1
* Storage: 128 GB, preferably SSDs
* Memory: 8 GB RAM (JVM default 4 GB)
* CPU: 4 cores or more

**REST Proxy**

* Nodes: 2 
* Storage: Only required for installation
* Memory: 1 GB overhead plus 64 MB per producer and 16 MB per consumer
* CPU: 16 cores to handle HTTP requests in parallel and background threads for consumers and producers.

**Replicator**
 
* Nodes: 2
* Storage: Only required for installation
* Memory: 0.5 - 4 GB heap size depending on connectors
* CPU: Typically not CPU- bound. More cores is better than faster cores.
* If deploying Confluent Platform on AWS VMs and running Replicator as a connector, be aware that VMs with burstable CPU types (T2, T3, T3a, and T4g) will not support high throughput streaming workloads.

You will need to prepare a bill of materials for software and hardware procurement. For each node type, these documents should address requirements for:

Number of nodes at each CPU level or cloud service tier

Data storage size and data redundancy strategy

Memory size

Networking bandwidth


== Additional resources

* https://eventsizer.io/granular[Cluster sizing calculators^]
* https://docs.confluent.io/platform/current/kafka/multi-node.html#cp-multi-node[Configure a multi-node enviroment^]
* https://docs.confluent.io/platform/current/installation/system-requirements.html#software[Software requirements^]

[.text-center]
Copyright © Confluent, Inc. 2014-2021. https://www.confluent.io/confluent-privacy-statement/[Privacy Policy] | https://www.confluent.io/terms-of-use/[Terms & Conditions]. +
Apache, Apache Kafka, Kafka and the Kafka logo are trademarks of the +
http://www.apache.org/[Apache Software Foundation]
